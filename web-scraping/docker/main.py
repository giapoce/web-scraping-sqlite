import csv
from bs4 import BeautifulSoup
import glob
import pandas as pd
import os
import requests
import urllib.parse
import psycopg2

html_files_glob = r"data-source\*htm"
csv_files_glob = r"csv-output\*csv"

sql_postgres_table = """ CREATE TABLE IF NOT EXISTS procedura_lotto (
                                       id integer PRIMARY KEY AUTOINCREMENT,
                                       lotto text NOT NULL,
                                       tipologia_procedura text,
                                       nr_procedura INTEGER,
                                       tipo_procedura TEXT,
                                       rito TEXT,
                                       pubblicazione_edicom TEXT,
                                       publicazione_portale TEXT,
                                       primo_identificativo INTEGER,
                                       ubicazione TEXT,
                                       genere TEXT,
                                       categoria TEXT,
                                       descrizione_sintetica TEXT,
                                       latitudine FLOAT8,
                                       longitudine FLOAT8
                                   ); """


def file_scrape(url):
    """ parse an html file to retrieve all relevant info
    about "procedura" and "lotto" sections and write them
    to a csv file
            :param
            :return
    """

    # Open a static html file and load its contents
    with open(url, 'r', encoding='UTF8') as f:
        contents = f.read()

    # Let's initialize the html parser
    soup = BeautifulSoup(contents, 'html.parser')

    # Header and data variables
    proc_csv_header = []
    proc_data = []
    lotto_csv_header = []
    lotto_data = []

    # Finding "procedura" section in the html file
    proc_card_tags = soup.find('h5', string='Dati sulla procedura')
    # Reading subsequent tags
    proc_next_elem = proc_card_tags.findNext()

    # Reading through all paragraphs in "proc_next_elem" to get fields name and values
    for elem in proc_next_elem.findAll('p'):

        if 'class' in elem.attrs:

            # Field name is a paragraph with a 'bg' class attached to it
            if elem.attrs['class'][0] == 'bg':
                # Let's do some cleansing
                cleansed_proc_header_field = " ".join(elem.get_text().strip().replace("\n", "").split())
                proc_csv_header.append(cleansed_proc_header_field)

        else:

            # Whatever else is a field value

            # Let's do some cleansing
            cleansed_proc_element = " ".join(elem.get_text().replace("\n", " ").split())
            proc_data.append(cleansed_proc_element)

    # Finding "lotto" section in the html file
    lotto_card_tags = soup.find('h5', string='Dati generali del lotto')
    # Reading subsequent tags
    lotto_next_elem = lotto_card_tags.findNext()

    # Reading from "lotto_next_elem" through all paragraphs or through all spans
    # whose "id" attribute is set to "genere_pvp" to get fields name and values
    for elem in lotto_next_elem.findAll(
            lambda tag: tag.name == 'p' or (tag.name == 'span' and tag.get('id') == 'genere_pvp')):

        if 'class' in elem.attrs:

            # Field name is a paragraph with a 'bg' class attached to it
            if elem.attrs['class'][0] == 'bg':
                # Let's do some cleansing
                cleansed_lotto_header_field = " ".join(elem.get_text().strip().replace("\n", "").split())
                lotto_csv_header.append(cleansed_lotto_header_field)

            elif elem.attrs['class'][0] == 'ubicazione':
                # Paragraph with 'ubicazione' class is a field value, not a field name
                
                # Let's do some cleansing
                cleansed_lotto_element = " ".join(elem.get_text().replace("\n", " ").split())
                lotto_data.append(cleansed_lotto_element)

        else:

            # Whatever else is a field value

            # Let's do some cleansing
            cleansed_lotto_element = " ".join(elem.get_text().replace("\n", " ").split())
            lotto_data.append(cleansed_lotto_element)

    # Combine headers and data from "procedura" and "lotto" sections
    header = proc_csv_header + lotto_csv_header
    data = proc_data + lotto_data

    # Creating a dataframe with collected data and headers
    df = pd.DataFrame(data=[data], columns=[header])

    # Writing all out to a csv file
    csv_file_name = url.replace("data-source","csv-output").replace(".htm", ".csv")
    df.to_csv(csv_file_name, index=False, quoting=csv.QUOTE_ALL)


def concat_csv_files():
    """ concatenate all csv files
        generated by main parsing loop
        :param
        :return: concatenated dataframe
    """

    # Remove unnecessary files
    files_to_remove = ['csv-output\{}'.format("all.csv"),'csv-output\{}'.format("coords.csv"),'csv-output\{}'.format("final.csv")]
    [os.remove(file) for file in files_to_remove if os.path.exists(file)]

    # Merge all csv files in a single csv file
    csv_files = glob.glob(csv_files_glob)
    df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)

    # Writing all out to a csv file and return dataframe
    df.to_csv(files_to_remove[0],index=False)
    return df


def retrieve_geo_coordinates(address):
    """ retrieve latitude and longitude from nominatim api for
        a given address
       :param address: lotto address
       :return: latitude and longitude
    """

    # Call nominatim api to retrieve latitude and longitude coordinates
    # corresponding to "address" parameter
    # The call may return nothing
    url = 'https://nominatim.openstreetmap.org/search/' + urllib.parse.quote(address) + '?format=json'
    response = requests.get(url).json()

    if len(response):
      return [response[0]["lat"],response[0]["lon"]]

    return [None,None]


def create_connection(**kwargs):
    """ create a database connection to a postgres database
       :param 
       :return: Connection object or None
    """

    # Open a connection to a local sqlite db whose file path is "db_file"
    # and return the connection object
    conn = None
    try:
        conn = psycopg2.connect(host=kwargs["host"],database=kwargs["database"],user=kwargs["user"],password=kwargs["password"])
        return conn
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
           conn.close()
           print('Database connection closed.')

    return conn


def create_table(conn, create_table_sql):
    """ create a table from the create_table_sql statement
        :param conn: Connection object
        :param create_table_sql: a CREATE TABLE statement
        :return:
    """

    # Open a cursor from a connection "conn" object and run the table creating ddl
    try:
        c = conn.cursor()
        c.execute(create_table_sql)

        cur.close()

        # commit the changes
        conn.commit()

    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
           conn.close()


def main():

    # Parsing all html files
    res = glob.glob(html_files_glob)
    for html_file in res:
        file_scrape(html_file)

    # Combine all csv files from previous step in a single csv file
    df=concat_csv_files()

    # Retrieve coordinates from nominatim api for each address in the dataframe "df"
    coords=[retrieve_geo_coordinates(address) for address in df["Ubicazione"]]

    # Generate a new DataFrame with address coordinates
    coords_df=pd.DataFrame(data=coords,columns=["latitude","longitude"])

    # Write "coords_df" DataFrame to a new csv file
    coords_df.to_csv('csv-output\{}'.format("coords.csv"),index=False, quoting=csv.QUOTE_ALL)

    # Concat old and new dataframe
    final_df=pd.concat([df,coords_df],axis="columns")

    # Write concatenated Data Frame to a new csv file
    final_df.to_csv('csv-output\{}'.format("final.csv"),index=False, quoting=csv.QUOTE_ALL)

    # Open connection to a postgres database
    conn = create_connection(host="localhost",database="postgres",user="postgres",password=os.getenv("POSTGRES_PASSWORD"))

    # Create table and load data frame into it
    if conn is not None:

        # create table to store data
        create_table(conn, sql_postgres_table)

        # Load data frame to the sqlite table
        final_df.to_sql("procedura_lotto", conn, if_exists='replace', index=False)

    else:
        print("Error! cannot create the database connection.")


if __name__ == '__main__':
    main()
